{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51fd0449",
   "metadata": {},
   "source": [
    "# 03 – Transformer Fine-Tuning (Linear Probe, One-Hot Path)\n",
    "\n",
    "This notebook trains a **linear probe** on top of a frozen encoder for binary regulatory DNA classification.\n",
    "\n",
    "**Design**\n",
    "- **Input format**: one-hot tensors `(N, L, 4)` saved in `data/processed/{train,val,test}.npz` with keys `X` and `y`.\n",
    "- **Projection**: a small `Linear(4 → hidden)` is used **only** when tokenizer is not used (one-hot path).\n",
    "- **Encoder**: optional Hugging Face model via `MODEL_NAME`; if loading fails or is omitted, a small placeholder encoder is used for quick tests.\n",
    "- **Safety**: DataLoaders use `num_workers=0` (notebook-safe); tensors are moved to device inside the training loop.\n",
    "- **Outputs**: checkpoints in `results/checkpoints/`, best model in `results/probe_best.pt`, metrics appended to `results/metrics.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7eab65fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, random, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "try:\n",
    "    from transformers import AutoModel, AutoTokenizer, AutoConfig, get_linear_schedule_with_warmup\n",
    "except Exception:\n",
    "    AutoModel = AutoTokenizer = AutoConfig = get_linear_schedule_with_warmup = None\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available(): torch.cuda.manual_seed_all(SEED)\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad27adc",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "da0fc24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "PROC = Path('data/processed')\n",
    "RESULTS = Path('results'); RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "CKPT_DIR = RESULTS / 'checkpoints'; CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Encoder model (optional). Leave empty to skip HF and use TinyEnc for one-hot runs.\n",
    "MODEL_NAME = os.environ.get('PRETRAINED_MODEL', '')  # '' means: use TinyEnc\n",
    "FULL_FINETUNE = False          # set True to unfreeze encoder\n",
    "MAX_LEN = 512                  # only relevant if tokenizer is used\n",
    "\n",
    "# Training hyperparameters\n",
    "EPOCHS = 3\n",
    "BATCH_TRAIN = 64\n",
    "BATCH_EVAL  = 128\n",
    "HEAD_LR = 1e-3\n",
    "ENCODER_LR = 1e-5\n",
    "WEIGHT_DECAY = 1e-2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e46ab",
   "metadata": {},
   "source": [
    "## Data loading (.npz with X/y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9733f626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: train=24 val=13 test=13\n"
     ]
    }
   ],
   "source": [
    "class SequenceDataset(Dataset):\n",
    "    \"\"\"Loads an .npz split containing either (X,y) or (inputs,labels).\n",
    "    This project uses one-hot, i.e., X=(N,L,4), y=(N,).\"\"\"\n",
    "    def __init__(self, path: str):\n",
    "        z = np.load(path, allow_pickle=True)\n",
    "        if 'X' in z and 'y' in z:\n",
    "            self.inputs, self.labels = z['X'], z['y']\n",
    "        elif 'inputs' in z and 'labels' in z:\n",
    "            # Fallback: text inputs (not used in one-hot path)\n",
    "            self.inputs, self.labels = z['inputs'], z['labels']\n",
    "        else:\n",
    "            raise ValueError(f'Unrecognized schema in {path}. Expected X/y.')\n",
    "        assert len(self.inputs)==len(self.labels), 'length mismatch'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
    "        return {'input': self.inputs[idx], 'label': int(self.labels[idx])}\n",
    "\n",
    "train_path = PROC/'train.npz'\n",
    "val_path   = PROC/'val.npz'\n",
    "test_path  = PROC/'test.npz'\n",
    "\n",
    "train_ds = SequenceDataset(str(train_path)) if train_path.exists() else None\n",
    "val_ds   = SequenceDataset(str(val_path))   if val_path.exists()   else None\n",
    "test_ds  = SequenceDataset(str(test_path))  if test_path.exists()  else None\n",
    "\n",
    "print('Datasets:',\n",
    "      f\"train={len(train_ds) if train_ds else None}\",\n",
    "      f\"val={len(val_ds) if val_ds else None}\",\n",
    "      f\"test={len(test_ds) if test_ds else None}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ff9e8c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train class 0: 12\n",
      "train class 1: 12\n",
      "val class 0: 6\n",
      "val class 1: 7\n",
      "test class 0: 7\n",
      "test class 1: 6\n",
      "Wrote label counts to results/preproc_summary.csv\n",
      "Computed pos_weight=1.0000 (neg=12, pos=12)\n"
     ]
    }
   ],
   "source": [
    "# Print label class counts for quick debug (why AUROC may be degenerate)\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "def get_labels(ds, path):\n",
    "    if ds is not None:\n",
    "        labels = getattr(ds, 'labels', None)\n",
    "        if labels is None:\n",
    "            try:\n",
    "                z = np.load(path, allow_pickle=True)\n",
    "                labels = z.get('y') if 'y' in z else z.get('labels')\n",
    "            except Exception:\n",
    "                return None\n",
    "        return np.asarray(labels).astype(int)\n",
    "    return None\n",
    "\n",
    "train_labels = get_labels(train_ds, train_path)\n",
    "val_labels = get_labels(val_ds, val_path)\n",
    "test_labels = get_labels(test_ds, test_path)\n",
    "\n",
    "splits = {'train': train_labels, 'val': val_labels, 'test': test_labels}\n",
    "\n",
    "# determine maximum class index across splits to create CSV columns\n",
    "max_class = 0\n",
    "for v in splits.values():\n",
    "    if v is not None and v.size > 0:\n",
    "        max_class = max(max_class, int(v.max()))\n",
    "\n",
    "ncols = max_class + 1\n",
    "rows = []\n",
    "for name, labels in splits.items():\n",
    "    if labels is None:\n",
    "        counts = [0] * ncols\n",
    "    else:\n",
    "        bc = np.bincount(labels, minlength=ncols)\n",
    "        counts = [int(x) for x in bc]\n",
    "    rows.append((name, counts))\n",
    "\n",
    "# print counts\n",
    "for name, counts in rows:\n",
    "    for i, c in enumerate(counts):\n",
    "        print(f\"{name} class {i}: {c}\")\n",
    "\n",
    "# write CSV summary (overwrite)\n",
    "out_path = RESULTS / 'preproc_summary.csv'\n",
    "header = ['split'] + [f'class_{i}' for i in range(ncols)]\n",
    "with open(out_path, 'w', newline='') as f:\n",
    "    w = csv.writer(f)\n",
    "    w.writerow(header)\n",
    "    for name, counts in rows:\n",
    "        w.writerow([name] + counts)\n",
    "\n",
    "print(f\"Wrote label counts to {out_path}\")\n",
    "\n",
    "# Compute pos_weight for BCEWithLogitsLoss (binary case) and expose to later cells\n",
    "pos_weight = None\n",
    "try:\n",
    "    if train_labels is not None:\n",
    "        uniq = np.unique(train_labels)\n",
    "        if uniq.size == 2:\n",
    "            neg = int((train_labels == 0).sum())\n",
    "            pos = int((train_labels == 1).sum())\n",
    "            if pos > 0:\n",
    "                import torch\n",
    "                pos_weight = torch.tensor(float(neg) / float(pos), dtype=torch.float32).to(DEVICE)\n",
    "                print(f\"Computed pos_weight={pos_weight.item():.4f} (neg={neg}, pos={pos})\")\n",
    "            else:\n",
    "                print('No positive examples in train set; pos_weight not set')\n",
    "        else:\n",
    "            print('pos_weight only computed for binary problems; found classes:', uniq)\n",
    "except Exception as e:\n",
    "    print('Could not compute pos_weight:', e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a4d8ea",
   "metadata": {},
   "source": [
    "## Collate function (one-hot aware, tokenizer-aware)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e65ffa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_collate_fn(tokenizer=None, max_length=512):\n",
    "    def collate(batch):\n",
    "        labels = torch.tensor([b['label'] for b in batch], dtype=torch.float)\n",
    "        xs = [b['input'] for b in batch]\n",
    "\n",
    "        # Tokenizer path (not used in one-hot, but supported)\n",
    "        if tokenizer is not None and isinstance(xs[0], (str, bytes)):\n",
    "            enc = tokenizer(list(xs), padding='longest', truncation=True,\n",
    "                             max_length=max_length, return_tensors='pt')\n",
    "            if 'attention_mask' not in enc:\n",
    "                enc['attention_mask'] = torch.ones_like(enc['input_ids'])\n",
    "            return {'input_ids': enc['input_ids'],\n",
    "                    'attention_mask': enc['attention_mask'],\n",
    "                    'label': labels}\n",
    "\n",
    "        # One-hot arrays: (L,4) → stack to (B,L,4)\n",
    "        arrs = [torch.as_tensor(x) for x in xs]\n",
    "        if arrs[0].ndim == 2 and arrs[0].shape[-1] == 4:\n",
    "            return {'embeddings': torch.stack(arrs), 'label': labels}\n",
    "\n",
    "        # 1D token ids (fallback)\n",
    "        if arrs[0].ndim == 1:\n",
    "            maxlen = max(a.shape[0] for a in arrs)\n",
    "            ids = torch.zeros((len(arrs), maxlen), dtype=torch.long)\n",
    "            for i,a in enumerate(arrs): ids[i,:a.shape[0]] = a\n",
    "            return {'input_ids': ids, 'attention_mask': (ids!=0).long(), 'label': labels}\n",
    "\n",
    "        # Fallback\n",
    "        return {'input': xs, 'label': labels}\n",
    "    return collate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7197da77",
   "metadata": {},
   "source": [
    "## Encoder and Linear Probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "064a922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF load failed, using tiny placeholder. Error: replace/with-safetensors-checkpoint is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`\n",
      "Encoder dim: 128 | PROJ: enabled\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "class LinearProbe(nn.Module):\n",
    "    def __init__(self, encoder: nn.Module, encoder_dim: int, freeze_encoder: bool = True, proj_in_features: int | None = None):\n",
    "        \"\"\"encoder: HF model or TinyEnc that returns last_hidden_state (B,L,H)\n",
    "        encoder_dim: H\n",
    "        proj_in_features: if not None, constructs a projection from proj_in_features -> encoder_dim\n",
    "                         and expects embeddings of shape (B,L,proj_in_features)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        if freeze_encoder and self.encoder is not None:\n",
    "            for p in self.encoder.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "        self.proj = nn.Linear(proj_in_features, encoder_dim).to(DEVICE) if proj_in_features is not None else None\n",
    "        if self.proj is not None:\n",
    "            # simple, stable init\n",
    "            nn.init.xavier_uniform_(self.proj.weight)\n",
    "            if self.proj.bias is not None:\n",
    "                nn.init.zeros_(self.proj.bias)\n",
    "\n",
    "        self.classifier = nn.Linear(encoder_dim, 1)\n",
    "\n",
    "    def forward(self, input_ids=None, attention_mask=None, embeddings=None):\n",
    "        if embeddings is None:\n",
    "            # Hugging Face encoder path\n",
    "            out = self.encoder(input_ids=input_ids, attention_mask=attention_mask, return_dict=True)\n",
    "            last_hidden = out.last_hidden_state\n",
    "        else:\n",
    "            last_hidden = embeddings  # expected (B,L,proj_in_features) if proj exists\n",
    "\n",
    "        # apply proj if present (applies to last dimension)\n",
    "        if self.proj is not None:\n",
    "            last_hidden = last_hidden.to(self.proj.weight.device).float()\n",
    "            last_hidden = self.proj(last_hidden)  # (B,L,H)\n",
    "\n",
    "        pooled = last_hidden.mean(dim=1)  # (B,H)\n",
    "        return self.classifier(pooled).squeeze(-1)\n",
    "\n",
    "\n",
    "def load_encoder(model_name: str, default_dim: int = 128):\n",
    "    \"\"\"Attempt to load a HF encoder; on failure return TinyEnc fallback and clear messages.\n",
    "    This function specially handles ModuleNotFoundError (e.g., missing triton) to give actionable guidance.\n",
    "    \"\"\"\n",
    "    class TinyEnc(nn.Module):\n",
    "        def __init__(self, dim=default_dim):\n",
    "            super().__init__()\n",
    "            self.dim = dim\n",
    "        def forward(self, input_ids=None, attention_mask=None, return_dict=True):\n",
    "            B = input_ids.shape[0] if input_ids is not None else 8\n",
    "            L = input_ids.shape[1] if input_ids is not None else 256\n",
    "            return type('X', (), {'last_hidden_state': torch.randn(B, L, self.dim, device=DEVICE)})\n",
    "\n",
    "    if not model_name:\n",
    "        return None, TinyEnc(default_dim).to(DEVICE), default_dim\n",
    "\n",
    "    if AutoModel is None:\n",
    "        warnings.warn(\"transformers package not available; using TinyEnc fallback.\")\n",
    "        return None, TinyEnc(default_dim).to(DEVICE), default_dim\n",
    "\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "        enc = AutoModel.from_pretrained(model_name, trust_remote_code=True)\n",
    "        cfg = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "        print('Loaded HF encoder:', model_name)\n",
    "        return tok, enc.to(DEVICE), getattr(cfg, 'hidden_size', default_dim)\n",
    "    except ModuleNotFoundError as e:\n",
    "        msg = str(e).lower()\n",
    "        if 'triton' in msg:\n",
    "            print(\n",
    "                \"HF model requires 'triton' which is not installed in this environment.\\n\"\n",
    "                \"Options:\\n\"\n",
    "                \"  1) Set PRETRAINED_MODEL='' (or None) to use the TinyEnc fallback for quick, notebook-safe runs.\\n\"\n",
    "                \"  2) Install triton following official instructions if you need the HF encoder (may require matching CUDA/toolkit).\\n\"\n",
    "                \"     Example: pip install triton  # verify CUDA/toolkit compatibility first.\\n\"\n",
    "                \"Note: Installing triton can be CUDA/version specific; avoid automatic installs inside the notebook.\" \n",
    "            )\n",
    "        else:\n",
    "            print('HF load failed with ModuleNotFoundError:', e)\n",
    "        return None, TinyEnc(default_dim).to(DEVICE), default_dim\n",
    "    except Exception as e:\n",
    "        print('HF load failed, using tiny placeholder. Error:', e)\n",
    "        return None, TinyEnc(default_dim).to(DEVICE), default_dim\n",
    "\n",
    "# load encoder and construct probe\n",
    "tokenizer, encoder, encoder_dim = load_encoder(MODEL_NAME)\n",
    "proj_in = 4 if tokenizer is None else None\n",
    "probe = LinearProbe(encoder, encoder_dim=encoder_dim, freeze_encoder=not FULL_FINETUNE, proj_in_features=proj_in).to(DEVICE)\n",
    "\n",
    "# Clear any legacy global PROJ variable to avoid double-projection in the running kernel\n",
    "PROJ = None\n",
    "\n",
    "print('Encoder dim:', encoder_dim, '| PROJ:', 'enabled' if proj_in is not None else 'disabled')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad18c9f",
   "metadata": {},
   "source": [
    "## DataLoaders (notebook-safe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a06a9e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'embeddings': torch.Size([24, 2000, 4]), 'label': torch.Size([24])}\n"
     ]
    }
   ],
   "source": [
    "if train_ds is not None:\n",
    "    collate = make_collate_fn(tokenizer, MAX_LEN)\n",
    "    # If we have training labels, create a WeightedRandomSampler to rebalance classes\n",
    "    sampler = None\n",
    "    try:\n",
    "        if 'train_labels' in globals() and train_labels is not None:\n",
    "            # small smoothing to avoid division by zero\n",
    "            counts = np.bincount(train_labels)\n",
    "            class_weights = 1.0 / (counts + 1e-8)\n",
    "            sample_weights = class_weights[train_labels]\n",
    "            sampler = torch.utils.data.WeightedRandomSampler(weights=sample_weights.tolist(),\n",
    "                                                            num_samples=len(sample_weights),\n",
    "                                                            replacement=True)\n",
    "    except Exception as e:\n",
    "        print('Could not build sampler:', e)\n",
    "        sampler = None\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=BATCH_TRAIN,\n",
    "                              shuffle=(sampler is None), sampler=sampler,\n",
    "                              collate_fn=collate, num_workers=0)\n",
    "    val_loader   = DataLoader(val_ds,   batch_size=BATCH_EVAL,  shuffle=False, collate_fn=collate, num_workers=0) if val_ds else None\n",
    "    test_loader  = DataLoader(test_ds,  batch_size=BATCH_EVAL,  shuffle=False, collate_fn=collate, num_workers=0) if test_ds else None\n",
    "    # Smoke test a batch\n",
    "    try:\n",
    "        b = next(iter(train_loader))\n",
    "        print({k: (v.shape if hasattr(v,'shape') else type(v)) for k,v in b.items()})\n",
    "    except Exception as e:\n",
    "        print('Smoke test failed:', e)\n",
    "else:\n",
    "    train_loader = val_loader = test_loader = None\n",
    "    print('No datasets found.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61b37c",
   "metadata": {},
   "source": [
    "## Train/Eval helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0378fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss: use pos_weight if available to rebalance BCEWithLogits\n",
    "try:\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight) if ('pos_weight' in globals() and pos_weight is not None) else nn.BCEWithLogitsLoss()\n",
    "except Exception:\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def forward_with_optional_proj(model, batch):\n",
    "    # Prefer model-local projection (model.proj) when available. Do not use a global PROJ variable.\n",
    "    if 'embeddings' in batch:\n",
    "        x = batch['embeddings'].to(DEVICE).float()  # (B,L,4) or (B,L,proj_in)\n",
    "        # Let the model handle projection internally if it has one\n",
    "        return model(embeddings=x)\n",
    "    else:\n",
    "        ids  = batch['input_ids'].to(DEVICE)\n",
    "        mask = batch['attention_mask'].to(DEVICE)\n",
    "        return model(input_ids=ids, attention_mask=mask)\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler=None):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for batch in tqdm(loader, leave=False):\n",
    "        y = batch['label'].to(DEVICE).float()\n",
    "        logits = forward_with_optional_proj(model, batch)\n",
    "        loss = criterion(logits, y)\n",
    "        optimizer.zero_grad(); loss.backward(); optimizer.step()\n",
    "        if scheduler is not None: scheduler.step()\n",
    "        losses.append(loss.item())\n",
    "    return float(np.mean(losses)) if losses else float('nan')\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_epoch(model, loader):\n",
    "    model.eval()\n",
    "    losses, ys, ps = [], [], []\n",
    "    for batch in tqdm(loader, leave=False):\n",
    "        y = batch['label'].to(DEVICE).float()\n",
    "        logits = forward_with_optional_proj(model, batch)\n",
    "        loss = criterion(logits, y)\n",
    "        p = torch.sigmoid(logits)\n",
    "        losses.append(loss.item())\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "        ps.append(p.detach().cpu().numpy())\n",
    "    if not ys: return float('nan'), float('nan'), float('nan')\n",
    "    y = np.concatenate(ys); p = np.concatenate(ps)\n",
    "    # Flatten\n",
    "    y = y.ravel(); p = p.ravel()\n",
    "    # If only one class present, AUROC/PR-AUC are undefined. Return NaN instead of raising.\n",
    "    unique_classes = np.unique(y)\n",
    "    if unique_classes.size < 2:\n",
    "        auroc = float('nan')\n",
    "        prauc = float('nan')\n",
    "    else:\n",
    "        auroc = roc_auc_score(y, p)\n",
    "        prauc = average_precision_score(y, p)\n",
    "    return float(np.mean(losses)), auroc, prauc\n",
    "\n",
    "def save_ckpt(model, path):\n",
    "    torch.save(model.state_dict(), path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685ba4b7",
   "metadata": {},
   "source": [
    "## Train the linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "87ece622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train_loss=0.6990 val_loss=0.6958 AUROC=0.5000 PR-AUC=0.5385 [0.0s] *\n",
      "Epoch 01 | train_loss=0.6959 val_loss=0.6954 AUROC=0.5000 PR-AUC=0.5385 [0.0s] \n",
      "Epoch 02 | train_loss=0.6956 val_loss=0.6951 AUROC=0.5000 PR-AUC=0.5385 [0.0s] \n",
      "Best validation: {'epoch': 0, 'val_auc': 0.5, 'val_ap': 0.5384615384615384}\n"
     ]
    }
   ],
   "source": [
    "if train_loader is not None:\n",
    "    # Build optimizer with correct param groups (classifier, optional proj, optional encoder)\n",
    "    def build_optimizer(probe, head_lr=HEAD_LR, enc_lr=ENCODER_LR, weight_decay=WEIGHT_DECAY, full_finetune=FULL_FINETUNE):\n",
    "        params = []\n",
    "        params.append({'params': probe.classifier.parameters(), 'lr': head_lr, 'weight_decay': weight_decay})\n",
    "        if getattr(probe, 'proj', None) is not None:\n",
    "            params.append({'params': probe.proj.parameters(), 'lr': head_lr, 'weight_decay': weight_decay})\n",
    "        if full_finetune:\n",
    "            enc_params = [p for p in probe.encoder.parameters() if p.requires_grad]\n",
    "            if len(enc_params):\n",
    "                params.append({'params': enc_params, 'lr': enc_lr, 'weight_decay': weight_decay})\n",
    "        return torch.optim.AdamW(params)\n",
    "\n",
    "    optimizer = build_optimizer(probe)\n",
    "\n",
    "    # Early stopping training loop\n",
    "    def train_one_epoch(model, loader, optimizer, device=DEVICE):\n",
    "        model.train()\n",
    "        losses = []\n",
    "        for batch in loader:\n",
    "            y = batch['label'].to(device).float()\n",
    "            logits = forward_with_optional_proj(model, batch)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Gradient clipping for stability on small/imbalanced splits\n",
    "            try:\n",
    "                torch.nn.utils.clip_grad_norm_(probe.parameters(), max_norm=1.0)\n",
    "            except Exception:\n",
    "                pass\n",
    "            optimizer.step()\n",
    "            losses.append(float(loss))\n",
    "        return float(np.mean(losses)) if losses else float('nan')\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def eval_metrics(model, loader, device=DEVICE):\n",
    "        if loader is None:\n",
    "            return float('nan'), float('nan'), float('nan')\n",
    "        model.eval()\n",
    "        losses, ys, ps = [], [], []\n",
    "        for batch in loader:\n",
    "            y = batch['label'].to(device).float()\n",
    "            logits = forward_with_optional_proj(model, batch)\n",
    "            loss = criterion(logits, y)\n",
    "            p = torch.sigmoid(logits)\n",
    "            losses.append(float(loss))\n",
    "            ys.append(y.detach().cpu().numpy())\n",
    "            ps.append(p.detach().cpu().numpy())\n",
    "        if not ys:\n",
    "            return float('nan'), float('nan'), float('nan')\n",
    "        y = np.concatenate(ys); p = np.concatenate(ps)\n",
    "        auroc = roc_auc_score(y, p) if np.unique(y).size > 1 else float('nan')\n",
    "        ap    = average_precision_score(y, p) if np.unique(y).size > 1 else float('nan')\n",
    "        return float(np.mean(losses)), auroc, ap\n",
    "\n",
    "    def fit_with_early_stopping(model, train_loader, val_loader, optimizer, epochs=EPOCHS, patience=5, ckpt_dir=CKPT_DIR, results_dir=RESULTS):\n",
    "        ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "        best = {'epoch': -1, 'val_auc': -np.inf, 'val_ap': 0.0}\n",
    "        wait = 0\n",
    "        history = []\n",
    "        for ep in range(epochs):\n",
    "            t0 = time.time()\n",
    "            tr_loss = train_one_epoch(model, train_loader, optimizer)\n",
    "            va_loss, va_auc, va_ap = eval_metrics(model, val_loader)\n",
    "            dt = time.time() - t0\n",
    "            ep_path = ckpt_dir / f'probe_epoch{ep}.pt'\n",
    "            save_ckpt(model, ep_path)\n",
    "            improved = not np.isnan(va_auc) and va_auc > best['val_auc']\n",
    "            if improved:\n",
    "                best = {'epoch': ep, 'val_auc': va_auc, 'val_ap': va_ap}\n",
    "                save_ckpt(model, results_dir / 'probe_best.pt')\n",
    "                wait = 0\n",
    "            else:\n",
    "                wait += 1\n",
    "            print(f\"Epoch {ep:02d} | train_loss={tr_loss:.4f} val_loss={va_loss:.4f} AUROC={va_auc:.4f} PR-AUC={va_ap:.4f} [{dt:.1f}s] {'*' if improved else ''}\")\n",
    "            # Append per-epoch metrics to CSV for later plotting/analysis\n",
    "            try:\n",
    "                epoch_log_path = RESULTS / \"epoch_metrics.csv\"\n",
    "                write_header = not epoch_log_path.exists()\n",
    "                with open(epoch_log_path, \"a\", newline=\"\") as f:\n",
    "                    w = csv.writer(f)\n",
    "                    if write_header:\n",
    "                        w.writerow([\"epoch\",\"train_loss\",\"val_loss\",\"val_auroc\",\"val_prauc\"])\n",
    "                    w.writerow([ep, f\"{tr_loss:.6f}\", f\"{va_loss:.6f}\", f\"{va_auc:.6f}\", f\"{va_ap:.6f}\"])\n",
    "            except Exception:\n",
    "                pass\n",
    "            history.append({'epoch': ep, 'train_loss': tr_loss, 'val_loss': va_loss, 'val_auc': va_auc, 'val_ap': va_ap})\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping (no val AUROC improvement in {patience} epochs).\")\n",
    "                break\n",
    "        # End for ep\n",
    "        # return results\n",
    "        print('Best validation:', best)\n",
    "        return best, history\n",
    "    # Run training\n",
    "    best, history = fit_with_early_stopping(probe, train_loader, val_loader, optimizer, epochs=EPOCHS, patience=5, ckpt_dir=CKPT_DIR, results_dir=RESULTS)\n",
    "    # Save run config snapshot for reproducibility\n",
    "    try:\n",
    "        import json, time\n",
    "        cfg = {\n",
    "            \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"seed\": int(SEED),\n",
    "            \"device\": str(DEVICE),\n",
    "            \"model_name\": MODEL_NAME or \"TinyEnc(one-hot)\",\n",
    "            \"full_finetune\": bool(FULL_FINETUNE),\n",
    "            \"epochs\": int(EPOCHS),\n",
    "            \"batch_train\": int(BATCH_TRAIN),\n",
    "            \"batch_eval\": int(BATCH_EVAL),\n",
    "            \"head_lr\": float(HEAD_LR),\n",
    "            \"encoder_lr\": float(ENCODER_LR),\n",
    "            \"weight_decay\": float(WEIGHT_DECAY),\n",
    "            \"train_n\": len(train_ds) if train_ds else 0,\n",
    "            \"val_n\": len(val_ds) if val_ds else 0,\n",
    "            \"test_n\": len(test_ds) if test_ds else 0\n",
    "        }\n",
    "        with open(RESULTS / \"run_config.json\", \"w\") as f:\n",
    "            json.dump(cfg, f, indent=2)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a47f21",
   "metadata": {},
   "source": [
    "## Final evaluation on test set & logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b3eb65c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST: AUROC=0.5000  (boot mean 0.5000, 95% CI [0.5000,0.5000])\n",
      "      PR-AUC=0.4615 (boot mean 0.4597, 95% CI [0.2288,0.7692])\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def collect_scores(model, loader, device=DEVICE):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for batch in loader:\n",
    "        y = batch['label'].to(device).float()\n",
    "        logits = forward_with_optional_proj(model, batch)\n",
    "        p = torch.sigmoid(logits)\n",
    "        ys.append(y.detach().cpu().numpy())\n",
    "        ps.append(p.detach().cpu().numpy())\n",
    "    if not ys:\n",
    "        return None, None\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "def bootstrap_ci(metric_fn, y, p, n_boot=1000, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(y)\n",
    "    vals = np.empty(n_boot, dtype=np.float64)\n",
    "    idx = np.arange(n)\n",
    "    for b in range(n_boot):\n",
    "        j = rng.choice(idx, size=n, replace=True)\n",
    "        try:\n",
    "            vals[b] = metric_fn(y[j], p[j])\n",
    "        except Exception:\n",
    "            vals[b] = np.nan\n",
    "    vals = vals[~np.isnan(vals)]\n",
    "    if len(vals) == 0:\n",
    "        return np.nan, (np.nan, np.nan)\n",
    "    lo, hi = np.percentile(vals, [2.5, 97.5])\n",
    "    return float(np.mean(vals)), (float(lo), float(hi))\n",
    "\n",
    "# Load best model for test scoring\n",
    "best_path = RESULTS / 'probe_best.pt'\n",
    "if best_path.exists():\n",
    "    probe.load_state_dict(torch.load(best_path, map_location=DEVICE))\n",
    "\n",
    "test_auc, test_ap = float('nan'), float('nan')\n",
    "if test_loader is not None:\n",
    "    y_test, p_test = collect_scores(probe, test_loader)\n",
    "    if y_test is not None:\n",
    "        # Point estimates\n",
    "        test_auc = roc_auc_score(y_test, p_test) if np.unique(y_test).size > 1 else float('nan')\n",
    "        test_ap  = average_precision_score(y_test, p_test) if np.unique(y_test).size > 1 else float('nan')\n",
    "        # Bootstrap CIs\n",
    "        auc_mean, (auc_lo, auc_hi) = bootstrap_ci(roc_auc_score, y_test, p_test, n_boot=1000, seed=SEED)\n",
    "        ap_mean,  (ap_lo,  ap_hi)  = bootstrap_ci(average_precision_score, y_test, p_test, n_boot=1000, seed=SEED)\n",
    "        print(f\"TEST: AUROC={test_auc:.4f}  (boot mean {auc_mean:.4f}, 95% CI [{auc_lo:.4f},{auc_hi:.4f}])\")\n",
    "        print(f\"      PR-AUC={test_ap:.4f} (boot mean {ap_mean:.4f}, 95% CI [{ap_lo:.4f},{ap_hi:.4f}])\")\n",
    "\n",
    "        # Append to results/metrics.csv\n",
    "        metrics_path = RESULTS / 'metrics.csv'\n",
    "        write_header = not metrics_path.exists()\n",
    "        with open(metrics_path, 'a', newline='') as f:\n",
    "            w = csv.writer(f)\n",
    "            if write_header:\n",
    "                w.writerow(['run_name','AUROC','PR_AUC','AUROC_CI_low','AUROC_CI_high',\n",
    "                            'PR_AUC_CI_low','PR_AUC_CI_high','split','best_epoch'])\n",
    "            w.writerow(['linear_probe',\n",
    "                        f'{test_auc:.4f}', f'{test_ap:.4f}',\n",
    "                        f'{auc_lo:.4f}', f'{auc_hi:.4f}',\n",
    "                        f'{ap_lo:.4f}', f'{ap_hi:.4f}',\n",
    "                        'test', best.get('epoch', -1)])\n",
    "else:\n",
    "    print('No test loader available.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c89daf",
   "metadata": {},
   "source": [
    "## Self-check: batch signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "cd105ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch signature:\n",
      "  embeddings torch.Size([24, 2000, 4]) torch.float32\n",
      "  label torch.Size([24]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "if train_loader is not None:\n",
    "    b = next(iter(train_loader))\n",
    "    print('Batch signature:')\n",
    "    for k,v in b.items():\n",
    "        if hasattr(v,'shape'):\n",
    "            print(' ', k, v.shape, v.dtype)\n",
    "        else:\n",
    "            print(' ', k, type(v))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
