{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b560b36",
   "metadata": {},
   "source": [
    "# Transformer Fine-Tuning for Regulatory DNA: Classifying Functional Elements and Variant Effects\n",
    "Angel Morenu  \n",
    "M.S. Applied Data Science, University of Florida  \n",
    "Email: angel.morenu@ufl.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc5aa89",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "This project evaluates transformer-based language models for DNA sequence analysis on regulatory genomics tasks. The aims are: (i) classify functional elements (promoters, enhancers, DNase-accessible regions) directly from sequence and (ii) prioritize the regulatory impact of noncoding variants. We will fine-tune DNABERT-2 and Nucleotide Transformer and benchmark against CNN baselines (DeepSEA, Basset, Basenji) and linear probes on frozen embeddings. Public datasets from ENCODE, Roadmap Epigenomics, and the DeepSEA training bundle provide standardized annotations (DNase, histone marks, TF binding). Primary metrics: AUROC and average precision (PR-AUC); secondary: compute cost (runtime, GPU memory) and cross-cell-type generalization. For variant effect prediction, in-silico mutagenesis will be used, with score distributions validated against published DeepSEA benchmarks. The purpose is to test whether transformer models better capture higher-order dependencies in DNA sequences than conventional CNNs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58eefd",
   "metadata": {},
   "source": [
    "## Plan of Action\n",
    "\n",
    "### A. What I Will Implement\n",
    "1) Data preprocessing pipeline for hg19/hg38 sequences: extract ±1kb windows, build labeled train/validation/test splits from ENCODE/DeepSEA annotations.  \n",
    "2) Baseline models:  \n",
    "- CNN baseline (Basset/Basenji minimal config)  \n",
    "- Linear probe on frozen transformer embeddings  \n",
    "3) Transformer models: fine-tune DNABERT-2 and Nucleotide Transformer with variable k-mer/BPE tokenization and context length.  \n",
    "4) Variant effect prediction: in-silico saturation mutagenesis on known regulatory loci.  \n",
    "5) Evaluation pipeline: AUROC, PR-AUC, bootstrap confidence intervals, runtime profiling, cross-cell-type transfer experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1b353",
   "metadata": {},
   "source": [
    "### B. Methods to Compare and Sources\n",
    "- DNABERT-2: https://huggingface.co/zhihan1996/DNABERT-2-117M  \n",
    "- Nucleotide Transformer (v1/v2): https://huggingface.co/InstaDeepAI/nucleotide-transformer-v2-50m-multi-species  \n",
    "- Basenji: https://github.com/calico/basenji  \n",
    "- Basset: https://github.com/davek44/Basset  \n",
    "- DeepSEA: https://deepsea.princeton.edu/help/ (portal: https://deepsea.princeton.edu/)  \n",
    "Optional ready-to-use models (Kipoi):  \n",
    "- https://kipoi.org/models/DeepSEA/predict/  \n",
    "- https://kipoi.org/models/DeepSEA/variantEffects/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc2565a",
   "metadata": {},
   "source": [
    "### C. Datasets and Sources\n",
    "- DeepSEA training bundle: http://deepsea.princeton.edu/help/  \n",
    "- ENCODE Project: https://www.encodeproject.org/  \n",
    "- Roadmap Epigenomics Data: https://egg2.wustl.edu/roadmap/web_portal/ (also via AWS Open Data and GEO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55bcfe7",
   "metadata": {},
   "source": [
    "### D. Experiments and Measurements\n",
    "- Multi-label classification (promoters, enhancers, DNase, TF binding, histone marks)  \n",
    "- Metrics: AUROC, PR-AUC, runtime, GPU memory, cross-cell-type generalization  \n",
    "- Variant effect prediction: mutagenesis scores compared with DeepSEA benchmarks (correlations, enrichment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9ac1d7",
   "metadata": {},
   "source": [
    "### E. Feasibility Considerations\n",
    "Large-scale transformer training is computationally intensive. We’ll use pre-trained checkpoints and fine-tune with smaller windows (1kb) on UF HPC or Colab Pro. CNN baselines (Basset/Basenji) remain lightweight for feasible comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a292811",
   "metadata": {},
   "source": [
    "## Preliminary Reading List\n",
    "1) Ji et al., DNABERT: Bioinformatics 2021.  \n",
    "2) Nguyen et al., DNABERT-2: bioRxiv 2023.  \n",
    "3) Dalla-Torre et al., Nucleotide Transformer: arXiv:2306.15006, 2023.  \n",
    "4) Zhou et al., DeepSEA: Nat Methods 2015.  \n",
    "5) Kelley et al., Basset: Genome Research 2016.  \n",
    "6) Avsec et al., Effective gene expression prediction from sequence by integrating long-range interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40163f85",
   "metadata": {},
   "source": [
    "## Notebook Index\n",
    "- 00 - Data Download: ./Data_download.ipynb\n",
    "- 01 – Data Preprocessing: ./01_data_preprocessing.ipynb  \n",
    "- 02 – Baseline Models: ./02_baseline_models.ipynb  \n",
    "- 03 – Transformer Fine-tuning: ./03_transformer_finetuning.ipynb  \n",
    "- 04 – Variant Effects: ./04_variant_effects.ipynb  \n",
    "- 05 – Results Visualization: ./05_results_visualization.ipynb"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
